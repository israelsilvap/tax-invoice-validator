import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
from imblearn.under_sampling import RandomUnderSampler
from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif
from modules.preprocessing import preprocessor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# -----------------------------------------------------------------------------
# ‚öôÔ∏è Fun√ß√µes Auxiliares para Treino e Avalia√ß√£o
# -----------------------------------------------------------------------------

def train_model(model, X_train, y_train, X_val, y_val):
    """Treina e avalia um modelo com exibi√ß√£o aprimorada no Streamlit."""
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    
    st.metric("üéØ Acur√°cia", f"{accuracy:.4f}")
    return model

def evaluate_model(model, X_val, y_val, model_name):
    """Avalia√ß√£o do desempenho do modelo com m√©tricas formatadas e gr√°ficos lado a lado."""
    y_pred = model.predict(X_val)
    y_proba = model.predict_proba(X_val)[:, 1]

    # Formatando o relat√≥rio de classifica√ß√£o em DataFrame
    report_dict = classification_report(y_val, y_pred, target_names=['valid', 'not valid'], output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose().round(3)

    # Criando layout com 3 colunas para melhor visualiza√ß√£o
    col1, col2, col3 = st.columns([0.8, 0.95, 1])

    with col1:
        st.markdown("### üìå **M√©tricas de Classifica√ß√£o**")
        st.dataframe(report_df.style.format("{:.3f}"))

    with col2:
        # Matriz de Confus√£o
        st.markdown("### üìå **Matriz de Confus√£o**")
        fig, ax = plt.subplots()  # Ajusta o tamanho da figura
        cm = confusion_matrix(y_val, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['V√°lido', 'Inv√°lido'], yticklabels=['V√°lido', 'Inv√°lido'])
        ax.set_xlabel("Predito")
        ax.set_ylabel("Real")
        ax.set_title(f'Matriz de Confus√£o')

        # Ajustar layout para ocupar o m√°ximo de espa√ßo poss√≠vel
        fig.tight_layout()
        st.pyplot(fig)

    with col3:
        st.markdown("### üìå **Curva ROC**")
        fig, ax = plt.subplots()  # Ajusta o tamanho da figura
        fpr, tpr, _ = roc_curve(y_val, y_proba)
        auc_score = roc_auc_score(y_val, y_proba)
        ax.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}', linewidth=2)
        ax.plot([0, 1], [0, 1], 'k--', linewidth=1)
        ax.set_xlabel('Falsos Positivos')
        ax.set_ylabel('Verdadeiros Positivos')
        ax.set_title('Curva ROC')
        ax.legend()

        # Ajustar layout para ocupar o m√°ximo de espa√ßo poss√≠vel
        fig.tight_layout()
        st.pyplot(fig)

@st.cache_data
def model_training_and_evaluation(df_clean):
    # -----------------------------------------------------------------------------
    # üìå Se√ß√£o 3.1: Divis√£o de Dados
    # -----------------------------------------------------------------------------
    with st.expander("üìå **Divis√£o de Dados**", expanded=False):
        st.markdown("""
        ### ‚ùì **Perguntas-chave**  
        - **Como dividir o dataset em treinamento, valida√ß√£o e teste?**  
        - **Por que usar divis√£o estratificada?**  
        """)

        # Separa√ß√£o em features (X) e alvo (y)
        X = df_clean.drop(columns=['class_label'])
        y = df_clean['class_label']

        # Aplicando Pipeline de Pr√©-processamento
        X = pd.DataFrame(preprocessor.fit_transform(X), columns=X.columns)

        # Divis√£o dos dados
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, stratify=y_train, random_state=42)

        # Exibir resumo da divis√£o dos dados
        st.markdown("### üìä **Resumo da Divis√£o dos Dados:**")
        st.write(f"- **Treinamento:** {len(X_train)} amostras ({len(X_train)/len(df_clean)*100:.2f}%)")
        st.write(f"- **Valida√ß√£o:** {len(X_val)} amostras ({len(X_val)/len(df_clean)*100:.2f}%)")
        st.write(f"- **Teste:** {len(X_test)} amostras ({len(X_test)/len(df_clean)*100:.2f}%)")

    # -----------------------------------------------------------------------------
    # üìå Se√ß√£o 3.2: Treinamento e Avalia√ß√£o dos Modelos
    # -----------------------------------------------------------------------------
    with st.expander("üìå **Treinamento e Avalia√ß√£o dos Modelos**", expanded=False):
        st.markdown("""
        ### ‚ùì **Perguntas-chave**  
        - **Quais modelos cl√°ssicos podemos testar?**  
        - **Como avaliar o desempenho com m√©tricas como acur√°cia e matriz de confus√£o?**  
        """)

                # Criando abas para os modelos
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["‚ñ´Ô∏è KNN", "‚ñ´Ô∏è SVM", "‚ñ´Ô∏è Naive Bayes", "‚ñ´Ô∏è XGBoost", "‚ñ´Ô∏è √Årvore de Decis√£o", "‚ñ´Ô∏è Random Forest"])

        # Modelo 1: KNN
        with tab1:
            knn_model = KNeighborsClassifier()
            knn_model = train_model(knn_model, X_train, y_train, X_val, y_val)
            evaluate_model(knn_model, X_val, y_val, "KNN")
        
        # Modelo 2: SVM
        with tab2:
            svm_model = SVC(probability=True, random_state=42)
            svm_model = train_model(svm_model, X_train, y_train, X_val, y_val)
            evaluate_model(svm_model, X_val, y_val, "SVM")

        # Modelo 3: Naive Bayes
        with tab3:
            nb_model = GaussianNB()
            nb_model = train_model(nb_model, X_train, y_train, X_val, y_val)
            evaluate_model(nb_model, X_val, y_val, "Naive Bayes")

        # Modelo 4: XGBoost
        with tab4:
            bst = XGBClassifier(random_state=42, n_jobs=-1)
            bst = train_model(bst, X_train, y_train, X_val, y_val)
            evaluate_model(bst, X_val, y_val, "XGBoost")

        # Modelo 5: √Årvore de Decis√£o
        with tab5:
            dt_model = DecisionTreeClassifier(random_state=42)
            dt_model = train_model(dt_model, X_train, y_train, X_val, y_val)
            evaluate_model(dt_model, X_val, y_val, "√Årvore de Decis√£o")

        # Modelo 6: Random Forest
        with tab6:
            rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
            rf_model = train_model(rf_model, X_train, y_train, X_val, y_val)
            evaluate_model(rf_model, X_val, y_val, "Random Forest")

    # =============================================================================
    # üèãÔ∏è Se√ß√£o 3.3: Balanceamento de Dados e Re-treinamento dos Modelos
    # =============================================================================
    with st.expander("üìå **Balanceamento de Dados e Re-treinamento**", expanded=False):
        st.markdown("""
        ### ‚ùì **Perguntas-chave**  
        - **O balanceamento da base melhora o desempenho dos modelos?**  
        - **Como os modelos se comportam ap√≥s a remo√ß√£o do vi√©s de classes?**  
        """)

        # -------------------------------------------------------------------------
        # üéØ Aplica√ß√£o do Balanceamento com RandomUnderSampler
        # -------------------------------------------------------------------------
        st.subheader("üìä **Balanceamento de Dados com RandomUnderSampler**")

        # Inicializar o RandomUnderSampler
        undersampler = RandomUnderSampler(random_state=42)

        # Aplicar o undersampling nos dados
        X_resampled, y_resampled = undersampler.fit_resample(X, y)

        # Encontrar as amostras removidas
        X_discarded = X[~X.index.isin(X_resampled.index)]
        y_discarded = y[~y.index.isin(y_resampled.index)]
        discarded_data = pd.concat([X_discarded, y_discarded], axis=1)

        # Aplicar o undersampling
        X, y = X_resampled, y_resampled

        # Exibir informa√ß√µes sobre a redu√ß√£o da base
        col1, col2, col3 = st.columns(3)
        col1.metric("üìâ Tamanho Antes", len(df_clean))
        col2.metric("üìä Tamanho Ap√≥s", len(X))
        col3.metric("üö´ Amostras Removidas", len(discarded_data))

        # -------------------------------------------------------------------------
        # üìå Divis√£o de Dados Ap√≥s Balanceamento
        # -------------------------------------------------------------------------
        st.subheader("üìä **Divis√£o de Dados Ap√≥s Balanceamento**")

        # Divis√£o estratificada
        X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)
        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.10, stratify=y_temp, random_state=42)

        # Exibir resumo das divis√µes
        col1, col2, col3 = st.columns(3)
        col1.metric("‚ñ´Ô∏è Treino", f"{len(X_train)} ({(len(X_train) / len(X) * 100):.2f}%)")
        col2.metric("‚ñ´Ô∏è Valida√ß√£o", f"{len(X_val)} ({(len(X_val) / len(X) * 100):.2f}%)")
        col3.metric("‚ñ´Ô∏è Teste", f"{len(X_test)} ({(len(X_test) / len(X) * 100):.2f}%)")

        # -------------------------------------------------------------------------
        # üìå Re-treinamento dos Modelos
        # -------------------------------------------------------------------------
        st.subheader("üìä **Re-treinamento dos Modelos**")

        # Criando abas para os modelos
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["‚ñ´Ô∏è SVM", "‚ñ´Ô∏è KNN",  "‚ñ´Ô∏è Naive Bayes", "‚ñ´Ô∏è XGBoost", "‚ñ´Ô∏è √Årvore de Decis√£o", "‚ñ´Ô∏è Random Forest"])

        # Modelo 1: SVM
        with tab1:
            svm_model = SVC(probability=True, random_state=42)
            svm_model = train_model(svm_model, X_train, y_train, X_val, y_val)
            evaluate_model(svm_model, X_val, y_val, "SVM")

        # Modelo 2: KNN
        with tab2:
            knn_model = KNeighborsClassifier(n_jobs=-1)
            knn_model = train_model(knn_model, X_train, y_train, X_val, y_val)
            evaluate_model(knn_model, X_val, y_val, "KNN")

        # Modelo 3: Naive Bayes
        with tab3:
            nb_model = GaussianNB()
            nb_model = train_model(nb_model, X_train, y_train, X_val, y_val)
            evaluate_model(nb_model, X_val, y_val, "Naive Bayes")

        # Modelo 4: XGBoost
        with tab4:
            bst = XGBClassifier(random_state=42, n_jobs=-1)
            bst = train_model(bst, X_train, y_train, X_val, y_val)
            evaluate_model(bst, X_val, y_val, "XGBoost")

        # Modelo 5: √Årvore de Decis√£o
        with tab5:
            dt_model = DecisionTreeClassifier(random_state=42)
            dt_model = train_model(dt_model, X_train, y_train, X_val, y_val)
            evaluate_model(dt_model, X_val, y_val, "√Årvore de Decis√£o")

        # Modelo 6: Random Forest
        with tab6:
            rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
            rf_model = train_model(rf_model, X_train, y_train, X_val, y_val)
            evaluate_model(rf_model, X_val, y_val, "Random Forest")


    # =============================================================================
    # üîç Se√ß√£o 3.4: Import√¢ncia das Features
    # =============================================================================
    with st.expander("üìå **Import√¢ncia das Features**", expanded=False):
        st.markdown("""
        ### ‚ùì **Perguntas-chave**  
        - **Quais vari√°veis mais influenciam a decis√£o dos modelos?**  
        - **Como podemos quantificar a relev√¢ncia de cada vari√°vel?**  
        """)

        # Criando colunas para dispor os gr√°ficos lado a lado
        col1, col2 = st.columns(2)

        feature_names = X.columns
        rf_importances = rf_model.feature_importances_
        sorted_idx = np.argsort(rf_importances)[-10:]  # Top 10 features

        df_corr = pd.concat([X, y], axis=1)
        correlation_matrix = df_corr.corr()
        corr_with_target = correlation_matrix['class_label'].sort_values(ascending=False)[1:]

        X_categoric = df_corr.drop('class_label', axis=1)
        mi = mutual_info_classif(X_categoric, df_corr['class_label'])
        mi_scores = pd.Series(mi, index=X_categoric.columns).sort_values(ascending=False)

        X_numeric = df_corr.drop(columns=['class_label'])
        y_numeric = df_corr['class_label']
        selector = SelectKBest(f_classif, k='all')
        selector.fit(X_numeric, y_numeric)
        anova_scores = pd.Series(selector.scores_, index=X_numeric.columns).sort_values(ascending=False)

        # -------------------------------------------------------------------------
        # üîç Feature Importance - Random Forest (Gr√°fico 1)
        # -------------------------------------------------------------------------
        with col1:
            st.markdown("### üìå **Import√¢ncia das Features - Random Forest**")
            fig, ax = plt.subplots(figsize=(7, 5))
            ax.barh(range(len(sorted_idx)), rf_importances[sorted_idx], align='center', color='skyblue')
            ax.set_yticks(range(len(sorted_idx)))
            ax.set_yticklabels([feature_names[i] for i in sorted_idx])
            ax.set_title("Top 10 Features Mais Importantes - Random Forest")
            ax.set_xlabel('Import√¢ncia', fontsize=12)
            st.pyplot(fig)

        # -------------------------------------------------------------------------
        #  ANOVA F-score (Gr√°fico 2)
        # -------------------------------------------------------------------------
        with col2:
            st.markdown("### üìå **ANOVA F-Score**")
            fig, ax = plt.subplots(figsize=(7, 5))
            x = np.arange(len(anova_scores))
            y = anova_scores.values
            ax.barh(x, y, color='lightcoral')
            ax.set_yticks(x)
            ax.set_yticklabels(anova_scores.index)
            ax.set_title("ANOVA F-Score das Vari√°veis Num√©ricas", fontsize=14)
            ax.set_xlabel("F-Score")
            st.pyplot(fig)

        # Criando a segunda linha de colunas
        col3, col4 = st.columns(2)

        # -------------------------------------------------------------------------
        # üß† Informa√ß√£o M√∫tua para Vari√°veis Categ√≥ricas (Gr√°fico 3)
        # -------------------------------------------------------------------------
        with col3:
            st.markdown("### üìå **Informa√ß√£o M√∫tua para Vari√°veis Categ√≥ricas**")
            fig, ax = plt.subplots(figsize=(10, 5))
            x = np.arange(len(mi_scores))
            y = mi_scores.values
            ax.bar(x, y, color='mediumseagreen')
            ax.set_title("Informa√ß√£o M√∫tua com 'class_label'", fontsize=14)
            ax.set_xticks(x)
            ax.set_xticklabels(mi_scores.index, rotation=45, ha='right')
            ax.set_ylabel("Informa√ß√£o M√∫tua")
            st.pyplot(fig)

        # -------------------------------------------------------------------------
        # üìà üìä Correla√ß√£o de Pearson (Gr√°fico 4)
        # -------------------------------------------------------------------------
        with col4:
            st.markdown("### üìå **Correla√ß√£o de Pearson**")
            fig, ax = plt.subplots(figsize=(10, 5.1))
            x = np.arange(len(corr_with_target))
            y = corr_with_target.values
            ax.bar(x, y, color='goldenrod')
            ax.set_title("Correla√ß√£o de Pearson com 'class_label'", fontsize=14)
            ax.set_xticks(x)
            ax.set_xticklabels(corr_with_target.index, rotation=45, ha='right')
            ax.set_ylabel("Correla√ß√£o")
            st.pyplot(fig)
    # =============================================================================
    # üéØ Se√ß√£o 3.6: Redu√ß√£o de Features e Re-treinamento
    # =============================================================================
    with st.expander("üìå **Redu√ß√£o de Features e Re-treinamento**", expanded=False):
        st.markdown("""
        ### ‚ùì **Pergunta-chave**  
        - **Quais benef√≠cios podemos obter ao reduzir o n√∫mero de vari√°veis?**  
        """)

        st.subheader("üìä **Sele√ß√£o das Top 5 Features**")
        
        # Sele√ß√£o das Top 5 Features
        selected_features = list(mi_scores[:5].index)
        
        # Criar DataFrame formatado
        top_features_df = pd.DataFrame({"Ranking": range(1, 6), "Feature": selected_features})

        # Exibi√ß√£o dos dados sem √≠ndice
        st.dataframe(top_features_df, hide_index=True)

        # Filtrando o conjunto de treino e valida√ß√£o com as features selecionadas
        X_train_filtered = X_train[selected_features]
        X_val_filtered = X_val[selected_features]
        X_test_filtered = X_test[selected_features]  # N√£o se esque√ßa de filtrar o conjunto de teste tamb√©m!

        # Criando abas para os modelos
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["‚ñ´Ô∏è Naive Bayes", "‚ñ´Ô∏è SVM", "‚ñ´Ô∏è KNN", "‚ñ´Ô∏è √Årvore de Decis√£o", "‚ñ´Ô∏è Random Forest", "‚ñ´Ô∏è XGBoost"])

        # Modelo 1: Naive Bayes
        with tab1:
            nb_model = GaussianNB()
            nb_model = train_model(nb_model, X_train_filtered, y_train, X_val_filtered, y_val)
            evaluate_model(nb_model, X_val_filtered, y_val, "Naive Bayes")

        # Modelo 2: SVM
        with tab2:
            svm_model = SVC(probability=True, random_state=42)
            svm_model = train_model(svm_model, X_train_filtered, y_train, X_val_filtered, y_val)
            evaluate_model(svm_model, X_val_filtered, y_val, "SVM")
        
        # Modelo 3: KNN
        with tab3:
            knn_model = KNeighborsClassifier(n_jobs=-1)
            knn_model = train_model(knn_model, X_train_filtered, y_train, X_val_filtered, y_val)
            evaluate_model(knn_model, X_val_filtered, y_val, "KNN")

        # Modelo 4: √Årvore de Decis√£o
        with tab4:
            dt_model = DecisionTreeClassifier(random_state=42)
            dt_model = train_model(dt_model, X_train_filtered, y_train, X_val_filtered, y_val)
            evaluate_model(dt_model, X_val_filtered, y_val, "√Årvore de Decis√£o")

        # Modelo 5: Random Forest
        with tab5:
            rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
            rf_model = train_model(rf_model, X_train_filtered, y_train, X_val_filtered, y_val)
            evaluate_model(rf_model, X_val_filtered, y_val, "Random Forest")

        # Modelo 6: XGBoost
        with tab6:
            bst = XGBClassifier(random_state=42, n_jobs=-1)
            bst = train_model(bst, X_train_filtered, y_train, X_val_filtered, y_val)
            evaluate_model(bst, X_val_filtered, y_val, "XGBoost")


    # =============================================================================
    # üìä Se√ß√£o 3.7: Avalia√ß√£o no Conjunto de Teste
    # =============================================================================
    with st.expander("üìå **Avalia√ß√£o no Conjunto de Teste**", expanded=False):
        st.markdown("""
        ### ‚ùì **Pergunta-chave**  
        - **Qual a performance final do melhor modelo no conjunto de teste independente?**  
        """)

        st.subheader("‚ñ´Ô∏è **Random Forest**")
        # Escolhendo o melhor modelo (exemplo: Random Forest)
        best_model = rf_model  
        X_test_filtered = X_test[selected_features]

        # Avalia√ß√£o no conjunto de teste
        evaluate_model(best_model, X_test_filtered, y_test, "Random Forest")

    return rf_model, preprocessor 
